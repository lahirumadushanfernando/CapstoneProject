{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "The Battle of the Neighborhoods - Week 2\nPart 1 Download and Explore New York city geographical coordinates dataset\nNeighborhood has a total of 5 boroughs and 306 neighborhoods. In order to segement the neighborhoods and explore them, we will essentially need a dataset that contains the 5 boroughs and the neighborhoods that exist in each borough as well as the the latitude and logitude coordinates of each neighborhood.\n\nLuckily, this dataset exists for free on the web. Link to the dataset: https://geo.nyu.edu/catalog/nyu_2451_34572\n\nFirst, let's download all the dependencies that we will need."}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\n!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nimport csv # implements classes to read and write tabular data in CSV form\n\nprint('Libraries imported.')", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Solving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda/envs/Python36\n\n  added / updated specs: \n    - geopy\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    geographiclib-1.50         |             py_0          34 KB  conda-forge\n    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n    geopy-1.22.0               |     pyh9f0ad1d_0          63 KB  conda-forge\n    openssl-1.1.1g             |       h516909a_0         2.1 MB  conda-forge\n    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n    certifi-2020.4.5.1         |   py36h9f0ad1d_0         151 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         2.5 MB\n\nThe following NEW packages will be INSTALLED:\n\n    geographiclib:   1.50-py_0           conda-forge\n    geopy:           1.22.0-pyh9f0ad1d_0 conda-forge\n    python_abi:      3.6-1_cp36m         conda-forge\n\nThe following packages will be UPDATED:\n\n    ca-certificates: 2020.1.1-0                      --> 2020.4.5.1-hecc5488_0     conda-forge\n    certifi:         2020.4.5.1-py36_0               --> 2020.4.5.1-py36h9f0ad1d_0 conda-forge\n    openssl:         1.1.1g-h7b6447c_0               --> 1.1.1g-h516909a_0         conda-forge\n\n\nDownloading and Extracting Packages\ngeographiclib-1.50   | 34 KB     | ##################################### | 100% \nca-certificates-2020 | 146 KB    | ##################################### | 100% \ngeopy-1.22.0         | 63 KB     | ##################################### | 100% \nopenssl-1.1.1g       | 2.1 MB    | ##################################### | 100% \npython_abi-3.6       | 4 KB      | ##################################### | 100% \ncertifi-2020.4.5.1   | 151 KB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nSolving environment: \\ ", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "!wget -q -O 'newyork_data.json' https://ibm.box.com/shared/static/fbpwbovar7lf8p5sgddm06cgipa2rxpe.json\nprint('Data downloaded!')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Load and explore the data"}, {"metadata": {}, "cell_type": "code", "source": "with open('newyork_data.json') as json_data:\n    newyork_data = json.load(json_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "All the relevant data is in the features key, which is basically a list of the neighborhoods. So, define a new variable that includes this data."}, {"metadata": {}, "cell_type": "code", "source": "neighborhoods_data = newyork_data['features']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Take a look at the first item in this list."}, {"metadata": {}, "cell_type": "code", "source": "neighborhoods_data[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Tranform the data into a pandas dataframe\nThe next task is essentially transforming this data of nested Python dictionaries into a pandas dataframe. Start by creating an empty dataframe."}, {"metadata": {}, "cell_type": "code", "source": "# define the dataframe columns\ncolumn_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n\n# instantiate the dataframe\nneighborhoods = pd.DataFrame(columns=column_names)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "neighborhoods", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Then loop through the data and fill the dataframe one row at a time."}, {"metadata": {}, "cell_type": "code", "source": "for data in neighborhoods_data:\n    borough = neighborhood_name = data['properties']['borough'] \n    neighborhood_name = data['properties']['name']\n        \n    neighborhood_latlon = data['geometry']['coordinates']\n    neighborhood_lat = neighborhood_latlon[1]\n    neighborhood_lon = neighborhood_latlon[0]\n    \n    neighborhoods = neighborhoods.append({'Borough': borough,\n                                          'Neighborhood': neighborhood_name,\n                                          'Latitude': neighborhood_lat,\n                                          'Longitude': neighborhood_lon}, ignore_index=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "neighborhoods.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's make sure that the dataset has all 5 boroughs and 306 neighborhoods."}, {"metadata": {}, "cell_type": "code", "source": "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n        len(neighborhoods['Borough'].unique()),\n        neighborhoods.shape[0]\n    )\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "neighborhoods.to_csv('BON1_NYC_GEO.csv',index=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use geopy library to get the latitude and longitude values of New York City."}, {"metadata": {}, "cell_type": "code", "source": "address = 'New York City, NY'\n\ngeolocator = Nominatim(user_agent=\"Jupyter\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of New York City are {}, {}.'.format(latitude, longitude))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create a map of New York with neighborhoods superimposed on top.\nFolium is a great visualization library. We can zoom into the below map, and click on each circle mark to reveal the name of the neighborhood and its respective borough."}, {"metadata": {}, "cell_type": "code", "source": "# create map of Toronto using latitude and longitude values\nmap_NewYork = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, borough, neighborhood in zip(neighborhoods['Latitude'], neighborhoods['Longitude'], neighborhoods['Borough'], neighborhoods['Neighborhood']):\n    label = '{}, {}'.format(neighborhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_NewYork)  \n    \nmap_NewYork", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Part 2 Web scrapping of Population and Demographics data of New York city from Wikipedia\nA : POPULATION DATA\nWeb scrapping of Population data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City"}, {"metadata": {}, "cell_type": "code", "source": "!pip install json", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\n\n# conda install -c anaconda beautiful-soup --yes\nfrom bs4 import BeautifulSoup # package for parsing HTML and XML documents\n\nimport csv # implements classes to read and write tabular data in CSV form\n\nprint('Libraries imported.')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Web scrapping of Population data from wikipedia page using BeautifulSoup.\nBeautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."}, {"metadata": {}, "cell_type": "code", "source": "website_url = requests.get('https://en.wikipedia.org/wiki/Demographics_of_New_York_City').text\nsoup = BeautifulSoup(website_url,'lxml')\ntable = soup.find('table',{'class':'wikitable sortable'})\n#print(soup.prettify())\n\nheaders = [header.text for header in table.find_all('th')]\n\ntable_rows = table.find_all('tr')        \nrows = []\nfor row in table_rows:\n   td = row.find_all('td')\n   row = [row.text for row in td]\n   rows.append(row)\n\nwith open('BON2_POPULATION1.csv', 'w') as f:\n   writer = csv.writer(f)\n   writer.writerow(headers)\n   writer.writerows(row for row in rows if row)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load data from CSV\nPop_data=pd.read_csv('BON2_POPULATION1.csv')\nPop_data.drop(Pop_data.columns[[7,8,9,10,11]], axis=1,inplace=True)\nprint('Data downloaded!')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Remoce whitespaces and rename columns\nPop_data.columns = Pop_data.columns.str.replace(' ', '')\nPop_data.columns = Pop_data.columns.str.replace('\\'','')\nPop_data.rename(columns={'Borough':'persons_sq_mi','County':'persons_sq_km'}, inplace=True)\nPop_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Pop_data.rename(columns = {'NewYorkCitysfiveboroughsvte\\n' : 'Borough',\n                   'Jurisdiction\\n':'County',\n                   'Population\\n':'Estimate_2017', \n                   'Landarea\\n':'square_miles',\n                    'Density\\n':'square_km'}, inplace=True)\nPop_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Replace newline('\\n') from each string from left and right sides\nPop_data['Borough']=Pop_data['Borough'].replace(to_replace='\\n', value='', regex=True)\nPop_data['County']=Pop_data['County'].replace(to_replace='\\n', value='', regex=True)\nPop_data['Estimate_2017']=Pop_data['Estimate_2017'].replace(to_replace='\\n', value='', regex=True)\nPop_data['square_miles']=Pop_data['square_miles'].replace(to_replace='\\n', value='', regex=True)\nPop_data['square_km']=Pop_data['square_km'].replace(to_replace='\\n', value='', regex=True)\nPop_data['persons_sq_mi']=Pop_data['persons_sq_mi'].replace(to_replace='\\n', value='', regex=True)\nPop_data['persons_sq_km']=Pop_data['persons_sq_km'].replace(to_replace='\\n', value='', regex=True)\nPop_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Shift data in the last two rows\nPop_data.loc[5:,['persons_sq_mi','persons_sq_km']] = Pop_data.loc[2:,['persons_sq_mi','persons_sq_km']].shift(1,axis=1)\nPop_data.loc[5:,['square_km','persons_sq_mi']] = Pop_data.loc[2:,['square_km','persons_sq_mi']].shift(1,axis=1)\nPop_data.loc[5:,['square_miles','square_km']] = Pop_data.loc[2:,['square_miles','square_km']].shift(1,axis=1)\nPop_data.loc[5:,['Estimate_2017','square_miles']] = Pop_data.loc[2:,['Estimate_2017','square_miles']].shift(1,axis=1)\nPop_data.loc[5:,['County','Estimate_2017']] = Pop_data.loc[2:,['County','Estimate_2017']].shift(1,axis=1)\nPop_data.loc[5:,['Borough','County']] = Pop_data.loc[2:,['Borough','County']].shift(1,axis=1)\nPop_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Remove 'NAN'\nPop_data = Pop_data.fillna('')\nPop_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Drop the last row\ni = Pop_data[((Pop_data.County == 'Sources: [2] and see individual borough articles'))].index\nPop_data.drop(i)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Save dataframe as csv file\nPop_data.to_csv('BON2_POPULATION.csv',index=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "B : DEMOGRAPHICS DATA\nWe will web scrap Demographics data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City\n\nWeb scrapping of Demographics data from wikipedia page using BeautifulSoup.\nBeautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."}, {"metadata": {}, "cell_type": "code", "source": "website_url = requests.get('https://en.wikipedia.org/wiki/New_York_City').text\nsoup = BeautifulSoup(website_url,'lxml')\ntable = soup.find('table',{'class':'wikitable sortable collapsible'})\n#print(soup.prettify())\n\nheaders = [header.text for header in table.find_all('th')]\n\ntable_rows = table.find_all('tr')        \nrows = []\nfor row in table_rows:\n   td = row.find_all('td')\n   row = [row.text for row in td]\n   rows.append(row)\n\nwith open('NYC_DEMO.csv', 'w') as f:\n   writer = csv.writer(f)\n   writer.writerow(headers)\n   writer.writerows(row for row in rows if row)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load data from CSV\nDemo_data=pd.read_csv('NYC_DEMO.csv')\nprint('Data downloaded!')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Demo_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Remove whitespaces and rename columns\nDemo_data.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Demo_data.rename(columns = {'2010[237]' : '2010',\n                   '1990[239]':'1990',\n                   '1970[239]':'1970', \n                   '1940[239]\\n':'1940',\n                    }, inplace=True)\nDemo_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Demo_data.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Demo_data.columns = Demo_data.columns.str.replace(' ', '')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Replace newline('\\n') from each string from left and right sides\nDemo_data= Demo_data.replace('\\n',' ', regex=True)\nDemo_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Strip '[240]' from third column - 1970\nDemo_data['1970'] = Demo_data['1970'].str.rstrip('[240]')\nDemo_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Demo_data.to_csv('BON2_DEMOGRAPHICS.csv',index=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Part 3 Download and Explore New York city and its Boroughs Cuisine dataset\nDownload all the dependencies that is need."}, {"metadata": {}, "cell_type": "code", "source": "!pip install PIL", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from PIL import Image # converting images into arrays\n\n%matplotlib inline\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.style.use('ggplot') # optional: for ggplot-like style\n\n# check for latest version of Matplotlib\nprint ('Matplotlib version: ', mpl.__version__) # >= 2.0.0\n\n# install wordcloud\n!conda install -c conda-forge wordcloud==1.4.1 --yes\n\n# import package and its set of stopwords\nfrom wordcloud import WordCloud, STOPWORDS\n\nprint ('Wordcloud is installed and imported!')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Fetch the file\nmy_file = project.get_file(\"BON3_NYC_CUISINE.csv\")\n\n# Read the CSV data file from the object storage into a pandas DataFrame\nmy_file.seek(0)\nimport pandas as pd\nNYC_CUISINE=pd.read_csv(my_file)\nNYC_CUISINE.drop(NYC_CUISINE.columns[[3,4,5,6,7]], axis=1,inplace=True) \nNYC_CUISINE.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "NYC_CUISINE.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(NYC_CUISINE.Borough.unique())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "CUISINE_WC = NYC_CUISINE[['Cuisine']]\nCUISINE_WC", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "CUISINE_WC.to_csv('CUISINE_WC.txt', sep=',', index=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "CUISINE_WC1 = open('CUISINE_WC.txt', 'r').read()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "stopwords = set(STOPWORDS)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# instantiate a word cloud object\nNYC_CUISINE_WC = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords\n)\n\n# generate the word cloud\nNYC_CUISINE_WC.generate(CUISINE_WC1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# display the word cloud\nplt.imshow(NYC_CUISINE_WC, interpolation='bilinear')\nplt.axis('off')\n\nfig = plt.figure()\nfig.set_figwidth(30)\nfig.set_figheight(45)\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Most Preferred Food in New York City -\n\nItalian\nPurto Rican\nMexican\nJewish\nIndian\nPakistani\nDominican"}, {"metadata": {}, "cell_type": "code", "source": "Brooklyn_data = NYC_CUISINE[NYC_CUISINE['Borough'] == 'Brooklyn'].reset_index(drop=True)\nBrooklyn_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BR_CUISINE_WC = Brooklyn_data[['Cuisine']]\nBR_CUISINE_WC", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BR_CUISINE_WC.to_csv('BR_CUISINE.txt', sep=',', index=False)\nBR_CUISINE_WC = open('BR_CUISINE.txt', 'r').read()\nstopwords = set(STOPWORDS)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# instantiate a word cloud object\nBR_CUISINE_NYC = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords\n)\n\n# generate the word cloud\nBR_CUISINE_NYC.generate(BR_CUISINE_WC)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# display the word cloud\nplt.imshow(BR_CUISINE_NYC, interpolation='bilinear')\nplt.axis('off')\n\nfig = plt.figure()\nfig.set_figwidth(30)\nfig.set_figheight(45)\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nMost Preferred Food in Brooklyn is -\n\nItalian\nPurto Rican\nMexican"}, {"metadata": {}, "cell_type": "code", "source": "Queens_data = NYC_CUISINE[NYC_CUISINE['Borough'] == 'Queens'].reset_index(drop=True)\nQueens_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Q_CUISINE_WC = Queens_data[['Cuisine']]\nQ_CUISINE_WC", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Q_CUISINE_WC.to_csv('Q_CUISINE.txt', sep=',', index=False)\nQ_CUISINE_WC = open('Q_CUISINE.txt', 'r').read()\nstopwords = set(STOPWORDS)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# instantiate a word cloud object\nQ_CUISINE_NYC = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords\n)\n\n# generate the word cloud\nQ_CUISINE_NYC.generate(Q_CUISINE_WC)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# display the word cloud\nplt.imshow(Q_CUISINE_NYC, interpolation='bilinear')\nplt.axis('off')\n\nfig = plt.figure()\nfig.set_figwidth(30)\nfig.set_figheight(45)\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nMost Preferred Food in Queens is -\n\nIndian\nIrish\nPakistani\nMexican"}, {"metadata": {}, "cell_type": "code", "source": "Bronx_data = NYC_CUISINE[NYC_CUISINE['Borough'] == 'The Bronx'].reset_index(drop=True)\nBronx_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BX_CUISINE_WC = Bronx_data[['Cuisine']]\nBX_CUISINE_WC", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BX_CUISINE_WC.to_csv('BX_CUISINE.txt', sep=',', index=False)\nBX_CUISINE_WC = open('BX_CUISINE.txt', 'r').read()\nstopwords = set(STOPWORDS)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# instantiate a word cloud object\nBX_CUISINE_NYC = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords\n)\n\n# generate the word cloud\nBX_CUISINE_NYC.generate(BX_CUISINE_WC)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# display the word cloud\nplt.imshow(BX_CUISINE_NYC, interpolation='bilinear')\nplt.axis('off')\n\nfig = plt.figure()\nfig.set_figwidth(30)\nfig.set_figheight(45)\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\nMost Preferred Food in The Bronx is - \n\nItalian\nPuerto Rican\nAlbanian\nDominican"}, {"metadata": {}, "cell_type": "markdown", "source": "Part 4 Download and Explore Farmers Market dataset\nDownload all the dependencies needed"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\n\n# notice: installing seaborn might takes a few minutes\n!conda install -c anaconda seaborn -y\nimport seaborn as sns\n\n!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nprint('Libraries imported.')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The dataset is downloaded from the website. The link is as given below :\n\nhttps://data.cityofnewyork.us/dataset/DOHMH-Farmers-Markets-and-Food-Boxes/8vwk-6iz2"}, {"metadata": {}, "cell_type": "code", "source": "# Data from website - https://data.cityofnewyork.us/dataset/DOHMH-Farmers-Markets-and-Food-Boxes/8vwk-6iz2\nmy_file = project.get_file(\"DOHMH_Farmers_Markets_and_Food_Boxes.csv\")\n\n# Read the CSV data file from the object storage into a pandas DataFrame\nmy_file.seek(0)\nFM_NYC=pd.read_csv(my_file)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "FM_NYC.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "FM_NYC.rename(columns={'Service Type':'Service_Type'}, inplace=True)\nprint(FM_NYC.Service_Type.unique())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "FM_NYC['Service_Type'].value_counts().to_frame()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig,ax = plt.subplots(1, 1, figsize=(5, 5))\nsns.countplot(x='Service_Type',data=FM_NYC)\nax.set_title(\"Service_Type\")\nfor t in ax.patches:\n    if (np.isnan(float(t.get_height()))):\n        ax.annotate('', (t.get_x(), 0))\n    else:\n        ax.annotate(str(format(int(t.get_height()), ',d')), (t.get_x(), t.get_height()*1.01))\n    \nplt.show();", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# FM_NYC_filtered - Dataset with only Farmers Market\nFM_NYC_filtered = FM_NYC[FM_NYC['Service_Type'] == 'Farmers Markets'].copy()\nFM_NYC_filtered ['Borough'] = FM_NYC_filtered['Borough'].map(lambda x: x.strip())\nprint(FM_NYC_filtered.shape)\nFM_NYC_filtered.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig,ax = plt.subplots(1, 1, figsize=(5, 5))\nsns.countplot(x='Borough',data=FM_NYC_filtered)\nax.set_title(\"Borough\")\nfor t in ax.patches:\n    if (np.isnan(float(t.get_height()))):\n        ax.annotate('', (t.get_x(), 0))\n    else:\n        ax.annotate(str(format(int(t.get_height()), ',d')), (t.get_x(), t.get_height()*1.01))\n        ax.set_xticklabels([t.get_text().split(\"T\")[0] for t in ax.get_xticklabels()])\n\n# This sets the yticks \"upright\" with 0, as opposed to sideways with 90.\nplt.xticks(rotation=90) \nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Manhattan amd Brooklyn has highest numbers of Farmers Markets\n\nUse geopy library to get the latitude and longitude values of New York City.\nThe geograpical coordinate of New York City are 40.7308619, -73.9871558."}, {"metadata": {}, "cell_type": "code", "source": "address = 'New York City, NY'\n\ngeolocator = Nominatim(user_agent=\"Jupyter\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of New York City are {}, {}.'.format(latitude, longitude))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Create a map of New York with Facility name and Borough of Farmers Market superimposed on top."}, {"metadata": {}, "cell_type": "code", "source": "# create map of New York City using latitude and longitude values\nmap_markets = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, FacilityName, borough in zip(FM_NYC_filtered['Latitude'], FM_NYC_filtered['Longitude'], FM_NYC_filtered['FacilityName'], FM_NYC_filtered['Borough']):\n            label = '{}, {}'.format(FacilityName, borough)\n            label = folium.Popup(label, parse_html=True)\n            folium.CircleMarker(\n                [lat, lng],\n                radius=5,\n                popup=label,\n                color='green',\n                fill=True,\n                fill_color='green',\n                fill_opacity=0.7,\n                parse_html = False).add_to(map_markets)  \n\nmap_markets", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Part 5A Segmenting and Clustering Neighborhoods - Brooklyn and Manhattan\nIntroduction\nIn this section of the capstone project, we will use the Foursquare API to explore neighborhoods in Brooklyn and Manhattan. We will use the explore function to get the most common venue categories in each neighborhood, and then use this feature to group the neighborhoods into clusters. We will use the k-means clustering algorithm to complete this task. Finally, we will use the Folium library to visualize the neighborhoods in Brooklyn and Manhattan and their emerging clusters.\n\nTable of Contents\n1. Download and Explore Dataset\n2. Explore Neighborhoods in Brooklyn and Manhattan\n3. Analyze Each Neighborhood\n4. Cluster Neighborhoods and Examine Clusters\n\nDownload all the dependencies that are needed."}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\n\n# import k-means from clustering stage\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.metrics import silhouette_score\n\n!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nprint('Libraries imported.')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "1. Download and Explore Dataset\nNeighborhood has a total of 5 boroughs and 306 neighborhoods. In order to segement the neighborhoods and explore them, we will essentially need a dataset that contains the 5 boroughs and the neighborhoods that exist in each borough as well as the latitude and logitude coordinates of each neighborhood.\n\nLoad and explore the data"}, {"metadata": {}, "cell_type": "code", "source": "NYC_Geo=pd.read_csv('BON1_NYC_GEO.csv')\nprint('Data downloaded!')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "NYC_Geo.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "NYC_Geo['Borough'].value_counts().to_frame()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "NYC_Geo.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(NYC_Geo.Borough.unique())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "NYC_Geo.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Segmenting and Clustering Neighborhoods - Brooklyn and Manhattan"}, {"metadata": {}, "cell_type": "code", "source": "BM_Geo = NYC_Geo.loc[(NYC_Geo['Borough'] == 'Brooklyn')|(NYC_Geo['Borough'] == 'Manhattan')]\nBM_Geo = BM_Geo.reset_index(drop=True)\nBM_Geo.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_Geo.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use geopy library to get the latitude and longitude values of New York City. The geograpical coordinate of New York City are 40.7308619, -73.9871558.."}, {"metadata": {}, "cell_type": "code", "source": "import time\nstart_time = time.time()\n\naddress = 'New York City, NY'\n\ngeolocator = Nominatim(user_agent=\"Jupyter\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('The geograpical coordinate of New York City are {}, {}.'.format(latitude, longitude))\n\nprint(\"--- %s seconds ---\" % round((time.time() - start_time), 2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create map of Toronto using latitude and longitude values\nmap_BM = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, borough, neighborhood in zip(BM_Geo['Latitude'], BM_Geo['Longitude'], BM_Geo['Borough'], BM_Geo['Neighborhood']):\n    label = '{}, {}'.format(neighborhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_BM)  \n    \nmap_BM", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "CLIENT_ID = 'OLN1BAQQBHO234LKFIU1ZNGV4Z3O3P1GS5KIMTNPJHLX1MKL' # your Foursquare ID\nCLIENT_SECRET = 'VDM5CGGVSUOGKMY21ETO4J1UAJH5QJEALQCJAIWUF2DJXR2T' # your Foursquare Secret\nVERSION = '20181218' # Foursquare API version\n\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "2. Explore Neighborhoods in Brooklyn and Manhattan\nExtract Venues data for each neighborhoods in Brooklyn"}, {"metadata": {}, "cell_type": "code", "source": "def getNearbyVenues(names, latitudes, longitudes, LIMIT=200, radius=1000):\n    \n    venues_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            LIMIT)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            name, \n            lat, \n            lng, \n            v['venue']['name'], \n            v['venue']['location']['lat'], \n            v['venue']['location']['lng'],  \n            v['venue']['categories'][0]['name']) for v in results])\n\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_venues)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Run the above function on each neighborhood and create a new dataframe called BM_venues."}, {"metadata": {}, "cell_type": "code", "source": "BM_venues = getNearbyVenues(names=BM_Geo['Neighborhood'],\n                                  latitudes=BM_Geo['Latitude'],\n                                  longitudes=BM_Geo['Longitude'],\n                                  LIMIT=200)\n\nprint('The \"BM_venues\" dataframe has {} venues and {} unique venue types.'.format(\n      len(BM_venues['Venue Category']),\n      len(BM_venues['Venue Category'].unique())))\n\nBM_venues.to_csv('BM_venues.csv', sep=',', encoding='UTF8')\nBM_venues.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "colnames = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']\nBM_venues = pd.read_csv('BM_venues.csv', skiprows=1, names=colnames)\nBM_venues.columns = BM_venues.columns.str.replace(' ', '')\nBM_venues.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_venues.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def Venues_Map(Borough_name, Borough_neighborhoods):\n    \n    # Use geopy library to get the latitude and longitude values \n    geolocator = Nominatim(user_agent=\"Jupyter\")\n    Borough_location = geolocator.geocode(Borough_name) #'Brooklyn, NY'\n    Borough_latitude = Borough_location.latitude\n    Borough_longitude = Borough_location.longitude\n    print('The geographical coordinates of \"{}\" are {}, {}.'.format(Borough_name, Borough_latitude, Borough_longitude))\n    \n    # To verify the number of Boroughs and Neighborhoods in the extracted data\n    print('The \"{}\" dataframe has {} different venue types and {} neighborhoods.'.format(\n          Borough_name,\n          len(Borough_neighborhoods['VenueCategory'].unique()),\n          len(Borough_neighborhoods['Neighborhood'].unique())))\n    \n    # create map of city using latitude and longitude values\n    map_Borough = folium.Map(location=[Borough_latitude, Borough_longitude], zoom_start=10)\n\n    # add markers to map\n    for lat, lng, venue, category in zip(Borough_neighborhoods['VenueLatitude'], Borough_neighborhoods['VenueLongitude'], Borough_neighborhoods['Venue'], Borough_neighborhoods['VenueCategory']):\n        label = '{}, {}'.format(category, venue)\n        label = folium.Popup(label, parse_html=True)\n        folium.CircleMarker(\n            [lat, lng],\n            radius=0.1,\n            popup=label,\n            color='red',\n            fill=True,\n            fill_color='#FF0000',\n            fill_opacity=0.3).add_to(map_Borough)  \n\n    return map_Borough", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Venues_Map('New York City, NY', BM_venues)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_venues.groupby('VenueCategory')['Venue'].count().sort_values(ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_venues.groupby('Neighborhood').count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print('There are {} uniques categories.'.format(len(BM_venues['VenueCategory'].unique())))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "3. Analyze Each Neighborhood"}, {"metadata": {}, "cell_type": "code", "source": "# one hot encoding\nBM_onehot = pd.get_dummies(BM_venues[['VenueCategory']], prefix=\"\", prefix_sep=\"\")\n\n#column lists before adding neighborhood\ncolumn_names = ['Neighborhood'] + list(BM_onehot.columns)\n\n# add neighborhood column back to dataframe\nBM_onehot['Neighborhood'] = BM_venues['Neighborhood'] \n\n# move neighborhood column to the first column\nBM_onehot = BM_onehot[column_names]\n\nBM_onehot.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "restaurant_List = []\nsearch = 'Restaurant'\nfor i in BM_onehot.columns :\n    if search in i:\n        restaurant_List.append(i)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "restaurant_List", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "col_name = []\ncol_name = ['Neighborhood'] + restaurant_List\nBM_restaurant = BM_onehot[col_name]\nBM_restaurant = BM_restaurant.iloc[:,1::]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_restaurant_grouped = BM_restaurant.groupby('Neighborhood').sum().reset_index()\nBM_restaurant_grouped['Total'] = BM_restaurant_grouped .sum(axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "4. Cluster Neighborhoods and Examine Clusters\nFirst, let's determine the optimal value of K for our dataset using the Silhouette Coefficient Method\n\nFrom sklearn documentation - https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n\nA higher Silhouette Coefficient score relates to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores: `\n\na: The mean distance between a sample and all other points in the same class.\n\nb: The mean distance between a sample and all other points in the next nearest cluster.\n\nThe Silhouette Coefficient is for a single sample is then given as:\n\ns=b-a/max(a,b)\n\nNow, to find the optimal value of k for KMeans, loop through 1..n for n_clusters in KMeans and calculate Silhouette Coefficient for each sample.\n\nA higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."}, {"metadata": {}, "cell_type": "code", "source": "BM_grouped_clustering = BM_restaurant_grouped.drop('Neighborhood', 1)\n\nfor n_cluster in range(2, 10):\n    kmeans = KMeans(n_clusters=n_cluster).fit(BM_grouped_clustering)\n    label = kmeans.labels_\n    sil_coeff = silhouette_score(BM_grouped_clustering, label, metric='euclidean')\n    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set number of clusters\nkclusters = 2\n\nBM_grouped_clustering = BM_restaurant_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(BM_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_results = pd.DataFrame(kmeans.cluster_centers_)\nBM_results.columns = BM_grouped_clustering.columns\nBM_results.index = ['cluster0','cluster1']\nBM_results['Total Sum'] = BM_results.sum(axis = 1)\nBM_results", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_results_merged = pd.DataFrame(BM_restaurant_grouped['Neighborhood'])\n\nBM_results_merged['Total'] = BM_restaurant_grouped['Total']\nBM_results_merged = BM_results_merged.assign(Cluster_Labels = kmeans.labels_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(BM_results_merged.shape)\nBM_results_merged", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BM_merged = BM_Geo\n\nBM_merged = BM_merged.join(BM_results_merged.set_index('Neighborhood'), on='Neighborhood')\n\nprint(BM_merged.shape)\nBM_merged.head(10) # check the last columns!", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": " create map\nmap_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i+x+(i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(BM_merged['Latitude'], BM_merged['Longitude'], BM_merged['Neighborhood'], BM_merged['Cluster_Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "List Neighborhoods of Interest in New York City\nCluster 1 : Saturated Markets"}, {"metadata": {}, "cell_type": "code", "source": "BM_merged[BM_merged['Cluster_Labels'] == 1].reset_index(drop=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Cluster 0 : Untapped Markets"}, {"metadata": {}, "cell_type": "code", "source": "BM_merged[BM_merged['Total'] == 0].reset_index(drop=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "There are no Untapped Markets for Restaurant business in Brooklyn and Manhattan.\n\nPart 5B Segmenting and Clustering Neighborhoods - Bronx, Queens and Staten Island\nIntroduction\nIn this section of the capstone project, we will use the Foursquare API to explore neighborhoods in Bronx, Queens and Staten Island.\n\nTable of Contents\n1. Download and Explore Dataset\n2. Explore Neighborhoods in Bronx, Queens and Staten Island\n3. Analyze Each Neighborhood\n4. Cluster Neighborhoods and Examine Clusters\n\n1. Download and Explore Dataset"}, {"metadata": {}, "cell_type": "code", "source": "BQS_Geo = NYC_Geo.loc[(NYC_Geo['Borough'] == 'Bronx')|(NYC_Geo['Borough'] == 'Queens')|(NYC_Geo['Borough'] == 'Staten Island')]\nBQS_Geo = BQS_Geo.reset_index(drop=True)\nBQS_Geo.head()\nBQS_Geo.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create map of Toronto using latitude and longitude values\nmap_BQS = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, borough, neighborhood in zip(BQS_Geo['Latitude'], BQS_Geo['Longitude'], BQS_Geo['Borough'], BQS_Geo['Neighborhood']):\n    label = '{}, {}'.format(neighborhood, borough)\n    label = folium.Popup(label, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=False).add_to(map_BQS)  \n    \nmap_BQS", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "2. Explore Neighborhoods in Bronx, Queens and Staten Island\u00b6\nRun the above function on each neighborhood and create a new dataframe called BQS_venues."}, {"metadata": {}, "cell_type": "code", "source": "BQS_venues = getNearbyVenues(names=BQS_Geo['Neighborhood'],\n                                  latitudes=BQS_Geo['Latitude'],\n                                  longitudes=BQS_Geo['Longitude'],\n                                  LIMIT=200)\n\nprint('The \"BQS_venues\" dataframe has {} venues and {} unique venue types.'.format(\n      len(BQS_venues['Venue Category']),\n      len(BQS_venues['Venue Category'].unique())))\n\nBQS_venues.to_csv('BQS_venues.csv', sep=',', encoding='UTF8')\nBQS_venues.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "colnames = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']\nBQS_venues = pd.read_csv('BQS_venues.csv', skiprows=1, names=colnames)\nBQS_venues.columns = BQS_venues.columns.str.replace(' ', '')\nBQS_venues.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Venues_Map('New York City, NY', BQS_venues)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BQS_venues.groupby('VenueCategory')['Venue'].count().sort_values(ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BQS_venues.groupby('Neighborhood').count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print('There are {} uniques categories.'.format(len(BQS_venues['VenueCategory'].unique())))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "3. Analyze Each Neighborhood"}, {"metadata": {}, "cell_type": "code", "source": "# one hot encoding\nBQS_onehot = pd.get_dummies(BQS_venues[['VenueCategory']], prefix=\"\", prefix_sep=\"\")\n\n#column lists before adding neighborhood\ncolumn_names = ['Neighborhood'] + list(BQS_onehot.columns)\n\n# add neighborhood column back to dataframe\nBQS_onehot['Neighborhood'] = BQS_venues['Neighborhood'] \n\n# move neighborhood column to the first column\nBQS_onehot = BQS_onehot[column_names]\n\nBQS_onehot.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "restaurant_List1 = []\nsearch = 'Restaurant'\nfor i in BQS_onehot.columns :\n    if search in i:\n        restaurant_List1.append(i)\n        \nrestaurant_List1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "col_name = []\ncol_name = ['Neighborhood'] + restaurant_List1\nBQS_restaurant = BQS_onehot[col_name]\nBQS_restaurant = BQS_restaurant.iloc[:,1::]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BQS_restaurant_grouped = BQS_restaurant.groupby('Neighborhood').sum().reset_index()\nBQS_restaurant_grouped['Total'] = BQS_restaurant_grouped .sum(axis=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n4. Cluster Neighborhoods and Examine Clusters\nFirst, let's determine the optimal value of K for our dataset using the Silhouette Coefficient Method"}, {"metadata": {}, "cell_type": "code", "source": "BQS_grouped_clustering = BQS_restaurant_grouped.drop('Neighborhood', 1)\n\nfor n_cluster in range(2, 10):\n    kmeans = KMeans(n_clusters=n_cluster).fit(BQS_grouped_clustering)\n    label = kmeans.labels_\n    sil_coeff = silhouette_score(BQS_grouped_clustering, label, metric='euclidean')\n    print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set number of clusters\nkclusters = 2\n\nBQS_grouped_clustering = BQS_restaurant_grouped.drop('Neighborhood', 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(BQS_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BQS_results = pd.DataFrame(kmeans.cluster_centers_)\nBQS_results.columns = BQS_grouped_clustering.columns\nBQS_results.index = ['cluster0','cluster1']\nBQS_results['Total Sum'] = BQS_results.sum(axis = 1)\nBQS_results", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BQS_results_merged = pd.DataFrame(BQS_restaurant_grouped['Neighborhood'],)\n\nBQS_results_merged['Total'] = BQS_restaurant_grouped['Total']\nBQS_results_merged = BQS_results_merged.assign(Cluster_Labels = kmeans.labels_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(BQS_results_merged.shape)\nBQS_results_merged", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "BQS_merged = BQS_Geo\n\nBQS_merged = BQS_merged.join(BQS_results_merged.set_index('Neighborhood'), on='Neighborhood')\n\nprint(BQS_merged.shape)\nBQS_merged.head(10) # check the last columns!", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create map\nmap_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i+x+(i*x)**2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(BQS_merged['Latitude'], BQS_merged['Longitude'], BQS_merged['Neighborhood'], BQS_merged['Cluster_Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "List Neighborhoods of Interest in New York City - Bronx, Queens and Staten Island\nCluster 1 : Saturated Markets"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}